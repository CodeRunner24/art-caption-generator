import json
import os
import random
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BlipProcessor
import numpy as np
from typing import Dict, List, Tuple


class ArtCapDataset(Dataset):
    """Art caption dataset for BLIP fine-tuning - Optimized for ArtCap format"""
    
    def __init__(self, json_path: str, images_dir: str, processor, split_ratio=None, split_type='train'):
        """
        Args:
            json_path: ArtCap.json dosyasÄ±nÄ±n yolu  
            images_dir: GÃ¶rsellerin bulunduÄŸu klasÃ¶r
            processor: BLIP processor
            split_ratio: (train, val, test) oranlarÄ±, default (0.8, 0.1, 0.1)
            split_type: 'train', 'val', veya 'test'
        """
        self.images_dir = images_dir
        self.processor = processor
        
        if split_ratio is None:
            split_ratio = (0.8, 0.1, 0.1)
        
        # JSON dosyasÄ±nÄ± yÃ¼kle
        print(f"ğŸ“‚ {json_path} yÃ¼kleniyor...")
        with open(json_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        # Dataset verification
        print(f"âœ… JSON'da {len(self.data)} gÃ¶rsel bulundu")
        
        # Mevcut gÃ¶rselleri kontrol et
        available_images = set(os.listdir(images_dir))
        print(f"âœ… KlasÃ¶rde {len(available_images)} gÃ¶rsel bulundu")
        
        # Her gÃ¶rsel iÃ§in tÃ¼m caption'larÄ± dÃ¼zenle
        self.image_caption_pairs = []
        missing_images = []
        
        for image_name, captions in self.data.items():
            if image_name in available_images:
                # Her caption iÃ§in ayrÄ± entry oluÅŸtur
                for caption in captions:
                    if caption.strip():  # BoÅŸ caption'larÄ± atla
                        self.image_caption_pairs.append((image_name, caption.strip()))
            else:
                missing_images.append(image_name)
        
        if missing_images:
            print(f"âš ï¸ {len(missing_images)} gÃ¶rsel klasÃ¶rde bulunamadÄ±")
        
        print(f"âœ… Toplam {len(self.image_caption_pairs)} image-caption Ã§ifti hazÄ±rlandÄ±")
        
        # Reproducible split iÃ§in seed ayarla
        random.seed(42)
        random.shuffle(self.image_caption_pairs)
        
        # Data split
        total_len = len(self.image_caption_pairs)
        train_end = int(total_len * split_ratio[0])
        val_end = train_end + int(total_len * split_ratio[1])
        
        if split_type == 'train':
            self.image_caption_pairs = self.image_caption_pairs[:train_end]
        elif split_type == 'val':
            self.image_caption_pairs = self.image_caption_pairs[train_end:val_end]
        elif split_type == 'test':
            self.image_caption_pairs = self.image_caption_pairs[val_end:]
        
        print(f"ğŸ¯ {split_type.upper()} dataset: {len(self.image_caption_pairs)} Ã¶rnekle")
        
        # Cache gÃ¶rsellerin boyutlarÄ±nÄ± kontrol et (ilk 5 Ã¶rnek)
        self._check_sample_images()
    
    def _check_sample_images(self):
        """Ä°lk birkaÃ§ gÃ¶rseli kontrol et"""
        print(f"ğŸ” Ã–rnek gÃ¶rselleri kontrol ediliyor...")
        
        for i in range(min(3, len(self.image_caption_pairs))):
            image_name, caption = self.image_caption_pairs[i]
            try:
                image_path = os.path.join(self.images_dir, image_name)
                image = Image.open(image_path)
                print(f"  âœ… {image_name}: {image.size} - {caption[:50]}...")
            except Exception as e:
                print(f"  âŒ {image_name}: HATA - {e}")
    
    def __len__(self):
        return len(self.image_caption_pairs)
    
    def __getitem__(self, idx):
        image_name, caption = self.image_caption_pairs[idx]
        
        # GÃ¶rseli yÃ¼kle
        try:
            image_path = os.path.join(self.images_dir, image_name)
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"âš ï¸ GÃ¶rsel yÃ¼kleme hatasÄ± {image_name}: {e}")
            # BoÅŸ beyaz gÃ¶rsel dÃ¶ndÃ¼r
            image = Image.new('RGB', (384, 384), color='white')
        
        return {
            'image': image,
            'caption': caption,
            'image_name': image_name
        }


def collate_fn(batch, processor):
    """BLIP fine-tuning iÃ§in optimize edilmiÅŸ collate function"""
    images = [item['image'] for item in batch]
    captions = [item['caption'] for item in batch]
    
    try:
        # Image preprocessing
        image_inputs = processor(
            images=images,
            return_tensors="pt",
            padding=True
        )
        
        # Text preprocessing - BLIP iÃ§in Ã¶zel format
        text_inputs = processor.tokenizer(
            captions,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=64  # Art caption'lar genelde kÄ±sa (avg 11 kelime)
        )
        
        # BLIP training batch format
        return {
            'pixel_values': image_inputs['pixel_values'],
            'input_ids': text_inputs['input_ids'],
            'attention_mask': text_inputs['attention_mask'],
            'labels': text_inputs['input_ids'].clone()  # Loss hesabÄ± iÃ§in
        }
        
    except Exception as e:
        print(f"âš ï¸ Collate function hatasÄ±: {e}")
        # Fallback: boÅŸ batch dÃ¶ndÃ¼r
        batch_size = len(batch)
        return {
            'pixel_values': torch.zeros(batch_size, 3, 384, 384),
            'input_ids': torch.zeros(batch_size, 10, dtype=torch.long),
            'attention_mask': torch.zeros(batch_size, 10, dtype=torch.long),
            'labels': torch.zeros(batch_size, 10, dtype=torch.long)
        }


def get_data_loaders(data_dir: str, processor, batch_size: int = 4):
    """Optimize edilmiÅŸ data loader'lar"""
    
    json_path = os.path.join(data_dir, 'ArtCap.json')
    images_dir = os.path.join(data_dir, 'ArtCap_Images_Dataset')
    
    print(f"ğŸ“Š Data loaders hazÄ±rlanÄ±yor...")
    print(f"  ğŸ“ JSON: {json_path}")
    print(f"  ğŸ“ GÃ¶rseller: {images_dir}")
    print(f"  ğŸ“¦ Batch size: {batch_size}")
    
    # Dataset'leri oluÅŸtur
    train_dataset = ArtCapDataset(json_path, images_dir, processor, split_type='train')
    val_dataset = ArtCapDataset(json_path, images_dir, processor, split_type='val')
    test_dataset = ArtCapDataset(json_path, images_dir, processor, split_type='test')
    
    # Custom collate function
    def collate_wrapper(batch):
        return collate_fn(batch, processor)
    
    # DataLoader'larÄ± oluÅŸtur - Mac optimizasyonu
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=0,  # Mac'te daha stabil
        pin_memory=False,  # MPS ile pin_memory problem Ã§Ä±karabiliyor
        collate_fn=collate_wrapper,
        drop_last=True  # Son incomplete batch'i atla
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=0, 
        pin_memory=False,
        collate_fn=collate_wrapper,
        drop_last=False
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=0, 
        pin_memory=False,
        collate_fn=collate_wrapper,
        drop_last=False
    )
    
    print(f"âœ… Data loaders hazÄ±r:")
    print(f"  ğŸš‚ Train: {len(train_loader)} batch ({len(train_dataset)} Ã¶rnek)")
    print(f"  âœ… Val: {len(val_loader)} batch ({len(val_dataset)} Ã¶rnek)")
    print(f"  ğŸ§ª Test: {len(test_loader)} batch ({len(test_dataset)} Ã¶rnek)")
    
    return train_loader, val_loader, test_loader


def setup_metal_device():
    """Metal GPU ayarlarÄ±nÄ± optimize et"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… Metal Performance Shaders (MPS) kullanÄ±lÄ±yor!")
        print(f"   MPS Backend: {torch.backends.mps.is_built()}")
        
        # MPS iÃ§in optimizasyonlar
        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
        
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("âœ… NVIDIA CUDA GPU kullanÄ±lÄ±yor!")
        print(f"   GPU: {torch.cuda.get_device_name()}")
    else:
        device = torch.device("cpu")
        print("âš ï¸ CPU kullanÄ±lÄ±yor - GPU'ya gÃ¶re yavaÅŸ olacak!")
    
    return device


def print_model_info(model):
    """Model bilgilerini detaylÄ± yazdÄ±r"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    print(f"\nğŸ¤– Model Architecture Bilgileri:")
    print(f"  ğŸ“Š Toplam parametre: {total_params:,}")
    print(f"  ğŸ¯ EÄŸitilecek parametre: {trainable_params:,}")
    print(f"  ğŸ§Š DondurulmuÅŸ parametre: {frozen_params:,}")
    print(f"  ğŸ“ˆ EÄŸitilecek oran: {100 * trainable_params / total_params:.1f}%")
    
    # Model komponenleri
    print(f"\nğŸ—ï¸ Model Komponenleri:")
    for name, child in model.named_children():
        child_params = sum(p.numel() for p in child.parameters())
        child_trainable = sum(p.numel() for p in child.parameters() if p.requires_grad)
        status = "ğŸ¯ TRAINABLE" if child_trainable > 0 else "ğŸ§Š FROZEN"
        print(f"  {status} {name}: {child_params:,} params")


def save_checkpoint(model, optimizer, epoch, loss, path):
    """Checkpoint kaydetme (metadata ile)"""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    import time
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'model_type': 'BLIP_ArtCaption_FineTuned',
        'timestamp': time.time()  # Unix timestamp
    }
    
    torch.save(checkpoint, path)
    print(f"ğŸ’¾ Checkpoint kaydedildi: {path}")
    print(f"   ğŸ“Š Epoch: {epoch}, Loss: {loss:.4f}")


def load_checkpoint(model, optimizer, path):
    """Checkpoint yÃ¼kleme (hata kontrolÃ¼ ile)"""
    try:
        print(f"ğŸ“‚ Checkpoint yÃ¼kleniyor: {path}")
        checkpoint = torch.load(path, map_location='cpu')
        
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        epoch = checkpoint.get('epoch', 0)
        loss = checkpoint.get('loss', float('inf'))
        
        print(f"âœ… Checkpoint yÃ¼klendi!")
        print(f"   ğŸ“Š Epoch: {epoch}, Loss: {loss:.4f}")
        
        return epoch, loss
        
    except Exception as e:
        print(f"âš ï¸ Checkpoint yÃ¼kleme hatasÄ±: {e}")
        return 0, float('inf') 