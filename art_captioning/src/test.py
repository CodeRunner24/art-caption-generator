import os
import torch
from PIL import Image
import random
from transformers import BlipForConditionalGeneration, BlipProcessor
from utils import setup_metal_device, ArtCapDataset
import matplotlib.pyplot as plt
import numpy as np


class ArtCaptionTester:
    def __init__(self, model_path=None, data_dir='data'):
        self.device = setup_metal_device()
        self.data_dir = data_dir
        
        # Model ve processor yÃ¼kle
        self.load_model(model_path)
        
        # Test dataset
        self.load_test_data()
    
    def load_model(self, model_path=None):
        """Model ve processor yÃ¼kle"""
        print("ğŸ”„ Model yÃ¼kleniyor...")
        
        # Processor yÃ¼kle
        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        
        if model_path and os.path.exists(model_path):
            print(f"ğŸ“‚ Fine-tuned model yÃ¼kleniyor: {model_path}")
            try:
                # Checkpoint yÃ¼kle
                checkpoint = torch.load(model_path, map_location='cpu')
                
                # Standart BLIP modelini yÃ¼kle
                self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
                
                # Fine-tuned weights yÃ¼kle
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                    print("âœ… Fine-tuned model baÅŸarÄ±yla yÃ¼klendi!")
                else:
                    print("âš ï¸ Checkpoint formatÄ± tanÄ±nmadÄ±, standart BLIP kullanÄ±lÄ±yor")
                    
            except Exception as e:
                print(f"âš ï¸ Fine-tuned model yÃ¼klenemedi: {e}")
                print("ğŸ”„ Standart BLIP modeliyle devam ediliyor...")
                self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        else:
            print("ğŸ”„ Standart BLIP modeli yÃ¼kleniyor...")
            self.model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        
        self.model = self.model.to(self.device)
        self.model.eval()
    
    def load_test_data(self):
        """Test dataset yÃ¼kle"""
        json_path = os.path.join(self.data_dir, 'ArtCap.json')
        images_dir = os.path.join(self.data_dir, 'ArtCap_Images_Dataset')
        
        self.test_dataset = ArtCapDataset(
            json_path, images_dir, self.processor, split_type='test'
        )
        print(f"ğŸ“Š Test dataset: {len(self.test_dataset)} Ã¶rnek")
    
    def generate_caption(self, image, use_beam_search=True, temperature=0.7):
        """Tek bir gÃ¶rsel iÃ§in caption generate et"""
        with torch.no_grad():
            # GÃ¶rsel preprocessing
            inputs = self.processor(images=image, return_tensors="pt")
            pixel_values = inputs['pixel_values'].to(self.device)
            
            if use_beam_search:
                # Beam search ile daha kaliteli sonuÃ§lar
                generated_ids = self.model.generate(
                    pixel_values,
                    max_length=100,
                    num_beams=5,
                    early_stopping=True,
                    no_repeat_ngram_size=2,
                    length_penalty=1.0
                )
            else:
                # Sampling ile daha Ã§eÅŸitli sonuÃ§lar
                generated_ids = self.model.generate(
                    pixel_values,
                    max_length=100,
                    do_sample=True,
                    temperature=temperature,
                    top_p=0.9,
                    early_stopping=True
                )
            
            caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)
            return caption
    
    def test_random_samples(self, num_samples=10):
        """Rastgele Ã¶rnekler Ã¼zerinde test"""
        print(f"\nğŸ¨ {num_samples} Rastgele Ã–rnek Test Ediliyor...")
        print("=" * 80)
        
        # Rastgele indeksler seÃ§
        indices = random.sample(range(len(self.test_dataset)), min(num_samples, len(self.test_dataset)))
        
        results = []
        
        for i, idx in enumerate(indices):
            print(f"\nğŸ“· Ã–rnek {i+1}/{num_samples}")
            print("-" * 40)
            
            try:
                # Dataset'ten Ã¶rnek al
                sample = self.test_dataset[idx]
                image_name = self.test_dataset.image_caption_pairs[idx][0]
                original_caption = self.test_dataset.image_caption_pairs[idx][1]
                
                # GÃ¶rseli yÃ¼kle
                image_path = os.path.join(self.data_dir, 'ArtCap_Images_Dataset', image_name)
                image = Image.open(image_path).convert('RGB')
                
                # Caption generate et
                print("ğŸ”„ Caption generate ediliyor...")
                generated_caption = self.generate_caption(image)
                
                print(f"ğŸ–¼ï¸ GÃ¶rsel: {image_name}")
                print(f"ğŸ“ Orijinal: {original_caption}")
                print(f"ğŸ¤– Generated: {generated_caption}")
                
                # SonuÃ§larÄ± kaydet
                results.append({
                    'image_name': image_name,
                    'original': original_caption,
                    'generated': generated_caption,
                    'image': image
                })
                
            except Exception as e:
                print(f"âš ï¸ Hata: {e}")
                continue
        
        return results
    
    def test_specific_image(self, image_path):
        """Belirli bir gÃ¶rsel Ã¼zerinde test"""
        print(f"\nğŸ–¼ï¸ Test ediliyor: {image_path}")
        print("-" * 50)
        
        try:
            # GÃ¶rseli yÃ¼kle
            image = Image.open(image_path).convert('RGB')
            
            # FarklÄ± generation stratejileri test et
            print("ğŸ”„ Caption'lar generate ediliyor...")
            
            beam_caption = self.generate_caption(image, use_beam_search=True)
            sample_caption = self.generate_caption(image, use_beam_search=False, temperature=0.7)
            creative_caption = self.generate_caption(image, use_beam_search=False, temperature=1.0)
            
            print(f"ğŸ¯ Beam Search: {beam_caption}")
            print(f"ğŸ² Sampling (T=0.7): {sample_caption}")
            print(f"ğŸ¨ Creative (T=1.0): {creative_caption}")
            
            return {
                'beam': beam_caption,
                'sample': sample_caption,
                'creative': creative_caption,
                'image': image
            }
            
        except Exception as e:
            print(f"âš ï¸ Hata: {e}")
            return None
    
    def evaluate_art_terminology(self, results):
        """Art-specific terminoloji kullanÄ±mÄ±nÄ± deÄŸerlendir"""
        print("\nğŸ¨ Art Terminoloji Analizi")
        print("=" * 50)
        
        # Art-specific terimler
        art_terms = {
            'style': ['abstract', 'realistic', 'impressionist', 'expressionist', 'cubist', 'surreal'],
            'medium': ['painting', 'oil', 'watercolor', 'acrylic', 'canvas', 'brush'],
            'composition': ['foreground', 'background', 'composition', 'perspective', 'depth'],
            'color': ['palette', 'hue', 'saturation', 'contrast', 'vibrant', 'muted'],
            'subject': ['portrait', 'landscape', 'still life', 'figure', 'nude'],
            'technique': ['brushstroke', 'texture', 'shading', 'lighting', 'chiaroscuro']
        }
        
        # Term kullanÄ±m istatistikleri
        term_usage = {}
        total_captions = len(results)
        
        for category, terms in art_terms.items():
            category_count = 0
            for result in results:
                caption = result['generated'].lower()
                if any(term in caption for term in terms):
                    category_count += 1
            
            usage_rate = (category_count / total_captions) * 100
            term_usage[category] = usage_rate
            print(f"ğŸ“Š {category.title()}: {usage_rate:.1f}% ({category_count}/{total_captions})")
        
        # Ortalama art terminology kullanÄ±mÄ±
        avg_usage = np.mean(list(term_usage.values()))
        print(f"\nğŸ¯ Ortalama Art Terminology KullanÄ±mÄ±: {avg_usage:.1f}%")
        
        return term_usage
    
    def create_results_visualization(self, results, save_path='outputs/test_results.png'):
        """Test sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir"""
        if not results:
            print("âš ï¸ GÃ¶rselleÅŸtirilecek sonuÃ§ yok")
            return
        
        num_results = min(6, len(results))  # Maksimum 6 Ã¶rnek gÃ¶ster
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('ğŸ¨ Art Caption Generation Results', fontsize=16, fontweight='bold')
        
        for i in range(num_results):
            row = i // 3
            col = i % 3
            
            if i < len(results):
                result = results[i]
                
                # GÃ¶rseli gÃ¶ster
                axes[row, col].imshow(result['image'])
                axes[row, col].axis('off')
                
                # Caption'larÄ± title olarak ekle
                title = f"ğŸ¤– {result['generated'][:50]}...\nğŸ“ {result['original'][:50]}..."
                axes[row, col].set_title(title, fontsize=10, pad=10, wrap=True)
            else:
                axes[row, col].axis('off')
        
        plt.tight_layout()
        
        # Kaydet
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š Test sonuÃ§larÄ± kaydedildi: {save_path}")
        plt.close()
    
    def compare_with_baseline(self, num_samples=20):
        """Fine-tuned model ile baseline modeli karÅŸÄ±laÅŸtÄ±r"""
        print("\nâš–ï¸ Baseline KarÅŸÄ±laÅŸtÄ±rma")
        print("=" * 50)
        
        # Baseline model yÃ¼kle
        print("ğŸ”„ Baseline BLIP model yÃ¼kleniyor...")
        baseline_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        baseline_model = baseline_model.to(self.device)
        baseline_model.eval()
        
        # Rastgele Ã¶rnekler seÃ§
        indices = random.sample(range(len(self.test_dataset)), min(num_samples, len(self.test_dataset)))
        
        comparisons = []
        
        for i, idx in enumerate(indices):
            try:
                sample = self.test_dataset[idx]
                image_name = self.test_dataset.image_caption_pairs[idx][0]
                
                # GÃ¶rseli yÃ¼kle
                image_path = os.path.join(self.data_dir, 'ArtCap_Images_Dataset', image_name)
                image = Image.open(image_path).convert('RGB')
                
                # Fine-tuned model caption
                fine_tuned_caption = self.generate_caption(image)
                
                # Baseline model caption
                with torch.no_grad():
                    inputs = self.processor(images=image, return_tensors="pt")
                    pixel_values = inputs['pixel_values'].to(self.device)
                    
                    generated_ids = baseline_model.generate(
                        pixel_values,
                        max_length=100,
                        num_beams=5,
                        early_stopping=True
                    )
                    
                    baseline_caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)
                
                comparisons.append({
                    'image_name': image_name,
                    'fine_tuned': fine_tuned_caption,
                    'baseline': baseline_caption
                })
                
                if i < 5:  # Ä°lk 5 Ã¶rneÄŸi gÃ¶ster
                    print(f"\nğŸ“· {image_name}")
                    print(f"ğŸ¨ Fine-tuned: {fine_tuned_caption}")
                    print(f"ğŸ”˜ Baseline: {baseline_caption}")
                
            except Exception as e:
                print(f"âš ï¸ KarÅŸÄ±laÅŸtÄ±rma hatasÄ±: {e}")
                continue
        
        return comparisons


def main():
    """Ana test fonksiyonu"""
    print("ğŸ§ª Art Caption Model Test")
    print("=" * 50)
    
    # Model yolunu belirle
    model_path = "outputs/best_model.pth"
    if not os.path.exists(model_path):
        print("âš ï¸ Fine-tuned model bulunamadÄ±, standart BLIP kullanÄ±lacak")
        model_path = None
    
    # Tester oluÅŸtur
    tester = ArtCaptionTester(model_path)
    
    # 1. Rastgele Ã¶rnekler test et
    results = tester.test_random_samples(10)
    
    # 2. Art terminology analizi
    if results:
        tester.evaluate_art_terminology(results)
    
    # 3. SonuÃ§larÄ± gÃ¶rselleÅŸtir
    if results:
        tester.create_results_visualization(results)
    
    # 4. Baseline ile karÅŸÄ±laÅŸtÄ±r (eÄŸer fine-tuned model varsa)
    if model_path:
        tester.compare_with_baseline(10)
    
    print("\nâœ… Test tamamlandÄ±!")


if __name__ == "__main__":
    main() 