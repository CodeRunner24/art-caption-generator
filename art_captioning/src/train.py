import os
import torch
import torch.nn as nn
from torch.optim import AdamW
from transformers import BlipForConditionalGeneration, BlipProcessor
from transformers import get_cosine_schedule_with_warmup
from tqdm import tqdm
import matplotlib.pyplot as plt
from utils import get_data_loaders, setup_metal_device, print_model_info, save_checkpoint
import time


class ArtCaptionTrainer:
    def __init__(self, config):
        print("ğŸš€ ArtCaptionTrainer baÅŸlatÄ±lÄ±yor...")
        self.config = config
        self.device = setup_metal_device()
        
        # Model ve processor yÃ¼kle
        self.load_model()
        
        # Data loaders - dataset yapÄ±sÄ±na optimize edilmiÅŸ
        print("\nğŸ“Š Data loaders hazÄ±rlanÄ±yor...")
        self.train_loader, self.val_loader, self.test_loader = get_data_loaders(
            config['data_dir'], 
            self.processor, 
            config['batch_size']
        )
        
        # Optimizer ve scheduler
        self.setup_optimizer()
        
        # Training history
        self.train_losses = []
        self.val_losses = []
        
        print("âœ… ArtCaptionTrainer hazÄ±r!")
    
    def load_model(self):
        """Pre-trained BLIP modelini yÃ¼kle"""
        print("ğŸ”„ BLIP modeli yÃ¼kleniyor...")
        
        # Ã–nce standart BLIP modelini yÃ¼kle
        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        
        # EÄŸer custom checkpoint varsa yÃ¼kle
        checkpoint_path = self.config['model_path']
        if os.path.exists(checkpoint_path):
            print(f"ğŸ“‚ Custom checkpoint yÃ¼kleniyor: {checkpoint_path}")
            try:
                # PyTorch checkpoint formatÄ±nda yÃ¼kle
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                # Checkpoint bir dict ise model state_dict al
                if isinstance(checkpoint, dict):
                    if 'model_state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                    elif 'state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['state_dict'], strict=False)
                    else:
                        # Dict'in kendisi state_dict olabilir
                        model.load_state_dict(checkpoint, strict=False)
                else:
                    # Checkpoint doÄŸrudan model ise
                    model = checkpoint
                    
                print("âœ… Custom checkpoint baÅŸarÄ±yla yÃ¼klendi!")
            except Exception as e:
                print(f"âš ï¸ Custom checkpoint yÃ¼klenemedi: {e}")
                print("ğŸ”„ Standart BLIP modeliyle devam ediliyor...")
        
        self.model = model.to(self.device)
        print_model_info(self.model)
    
    def setup_optimizer(self):
        """Optimizer ve learning rate scheduler ayarla"""
        # Model attributelarÄ±nÄ± kontrol et
        print("ğŸ” Model attributelarÄ±:")
        for name, _ in self.model.named_children():
            print(f"   - {name}")
        
        # Sadece belirli layer'larÄ± fine-tune et (memory efficiency iÃ§in)
        trainable_params = []
        
        # Vision encoder'Ä± dondurmaya bÄ±rak (memory tasarrufu)
        if hasattr(self.model, 'vision_model'):
            for param in self.model.vision_model.parameters():
                param.requires_grad = False
        
        # Text decoder varsa fine-tune et
        if hasattr(self.model, 'text_decoder'):
            for param in self.model.text_decoder.parameters():
                param.requires_grad = True
                trainable_params.append(param)
        
        # Language model varsa fine-tune et
        if hasattr(self.model, 'language_model'):
            for param in self.model.language_model.parameters():
                param.requires_grad = True
                trainable_params.append(param)
        
        # EÄŸer hiÃ§ trainable param yoksa tÃ¼m modeli train et
        if len(trainable_params) == 0:
            print("âš ï¸ HiÃ§ trainable parametre bulunamadÄ±, tÃ¼m modeli fine-tune ediliyor...")
            trainable_params = list(self.model.parameters())
            for param in trainable_params:
                param.requires_grad = True
        
        print(f"ğŸ¯ Fine-tune edilecek parametre sayÄ±sÄ±: {sum(p.numel() for p in trainable_params):,}")
        
        self.optimizer = AdamW(trainable_params, lr=self.config['learning_rate'], weight_decay=0.01)
        
        # Cosine scheduler
        total_steps = len(self.train_loader) * self.config['epochs']
        warmup_steps = int(0.1 * total_steps)  # Ä°lk %10'da warm-up
        
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
    
    def train_epoch(self, epoch):
        """Bir epoch eÄŸitim"""
        self.model.train()
        total_loss = 0
        
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}")
        
        for batch_idx, batch in enumerate(progress_bar):
            try:
                # Batch'i device'a taÅŸÄ±
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # Forward pass
                self.optimizer.zero_grad()
                outputs = self.model(**batch)
                loss = outputs.loss
                
                # Loss None kontrolÃ¼
                if loss is None:
                    print(f"âš ï¸ Batch {batch_idx} - Loss is None, skipping...")
                    continue
                
                # Backward pass
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                self.scheduler.step()
                
                total_loss += loss.item()
                
                # Progress bar gÃ¼ncelle
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'LR': f'{self.scheduler.get_last_lr()[0]:.2e}',
                    'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'
                })
                
                # Metal GPU memory management
                if self.device.type == 'mps':
                    torch.mps.empty_cache()
                
            except Exception as e:
                print(f"âš ï¸ Batch {batch_idx} hatasÄ±: {e}")
                continue
        
        return total_loss / len(self.train_loader)
    
    def validate(self):
        """Validation"""
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation"):
                try:
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    outputs = self.model(**batch)
                    total_loss += outputs.loss.item()
                except Exception as e:
                    print(f"âš ï¸ Validation batch hatasÄ±: {e}")
                    continue
        
        return total_loss / len(self.val_loader) if len(self.val_loader) > 0 else float('inf')
    
    def generate_sample_captions(self, num_samples=3):
        """Ã–rnek caption'lar generate et"""
        self.model.eval()
        print("\nğŸ¨ Ã–rnek Caption'lar:")
        print("-" * 50)
        
        with torch.no_grad():
            for i, batch in enumerate(self.test_loader):
                if i >= num_samples:
                    break
                
                try:
                    # Ä°lk gÃ¶rseli al
                    pixel_values = batch['pixel_values'][:1].to(self.device)
                    
                    # Caption generate et
                    generated_ids = self.model.generate(
                        pixel_values,
                        max_length=50,
                        num_beams=4,
                        early_stopping=True,
                        do_sample=True,
                        temperature=0.7
                    )
                    
                    generated_caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)
                    
                    # Original caption
                    if 'labels' in batch:
                        original_ids = batch['labels'][:1]
                        original_caption = self.processor.decode(original_ids[0], skip_special_tokens=True)
                        print(f"Ã–rnek {i+1}:")
                        print(f"  Orijinal: {original_caption}")
                        print(f"  Generated: {generated_caption}")
                    else:
                        print(f"Ã–rnek {i+1} Generated: {generated_caption}")
                    
                    print()
                
                except Exception as e:
                    print(f"âš ï¸ Caption generation hatasÄ±: {e}")
                    continue
    
    def train(self):
        """Ana training loop"""
        print("ğŸš€ Training baÅŸlÄ±yor...")
        print(f"ğŸ“Š Training samples: {len(self.train_loader.dataset)}")
        print(f"ğŸ“Š Validation samples: {len(self.val_loader.dataset)}")
        print(f"ğŸ“Š Batch size: {self.config['batch_size']}")
        print(f"ğŸ“Š Epochs: {self.config['epochs']}")
        print(f"ğŸ“Š Learning rate: {self.config['learning_rate']}")
        
        best_val_loss = float('inf')
        
        for epoch in range(self.config['epochs']):
            print(f"\nğŸ”„ Epoch {epoch+1}/{self.config['epochs']}")
            
            # Training
            train_loss = self.train_epoch(epoch)
            self.train_losses.append(train_loss)
            
            # Validation
            val_loss = self.validate()
            self.val_losses.append(val_loss)
            
            print(f"ğŸ“ˆ Train Loss: {train_loss:.4f}")
            print(f"ğŸ“‰ Val Loss: {val_loss:.4f}")
            
            # En iyi modeli kaydet
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_model_path = os.path.join(self.config['output_dir'], 'best_model.pth')
                save_checkpoint(self.model, self.optimizer, epoch, val_loss, best_model_path)
            
            # Her epoch sonunda Ã¶rnek caption'lar gÃ¶ster
            if (epoch + 1) % 1 == 0:
                self.generate_sample_captions()
            
            # Checkpoint kaydet
            checkpoint_path = os.path.join(self.config['output_dir'], f'checkpoint_epoch_{epoch+1}.pth')
            save_checkpoint(self.model, self.optimizer, epoch, val_loss, checkpoint_path)
        
        # Training tamamlandÄ±
        print("\nğŸ‰ Training tamamlandÄ±!")
        print(f"ğŸ’¾ En iyi model: {best_model_path}")
        
        # Loss grafiÄŸi Ã§iz
        self.plot_training_history()
    
    def plot_training_history(self):
        """Training history grafiÄŸi"""
        plt.figure(figsize=(10, 6))
        plt.plot(self.train_losses, label='Train Loss', color='blue')
        plt.plot(self.val_losses, label='Validation Loss', color='red')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Art Caption Training History')
        plt.legend()
        plt.grid(True)
        
        # Grafik kaydet
        plot_path = os.path.join(self.config['output_dir'], 'training_history.png')
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š Training grafiÄŸi kaydedildi: {plot_path}")
        plt.close()


def main():
    """Ana fonksiyon"""
    # Config
    config = {
        'data_dir': 'data',
        'model_path': 'models/model_base_capfilt_large.pth',
        'output_dir': 'outputs',
        'batch_size': 4,  # Dataset optimize edildi, batch size artÄ±rÄ±ldÄ±
        'learning_rate': 1e-4,  # Learning rate artÄ±rÄ±ldÄ± (0.0001) - art data iÃ§in optimum
        'epochs': 5,
    }
    
    # File path'leri detaylÄ± print et
    print("\nğŸ“ KULLANILAN DOSYA YOLLARI:")
    print(f"  ğŸ—‚ï¸  Data klasÃ¶rÃ¼: {os.path.abspath(config['data_dir'])}")
    print(f"  ğŸ“Š JSON dosyasÄ±: {os.path.abspath(os.path.join(config['data_dir'], 'ArtCap.json'))}")
    print(f"  ğŸ–¼ï¸  GÃ¶rsel klasÃ¶rÃ¼: {os.path.abspath(os.path.join(config['data_dir'], 'ArtCap_Images_Dataset'))}")
    print(f"  ğŸ¤– Model dosyasÄ±: {os.path.abspath(config['model_path'])}")
    print(f"  ğŸ’¾ Output klasÃ¶rÃ¼: {os.path.abspath(config['output_dir'])}")
    
    # File kontrolÃ¼
    json_path = os.path.join(config['data_dir'], 'ArtCap.json')
    images_dir = os.path.join(config['data_dir'], 'ArtCap_Images_Dataset')
    model_path = config['model_path']
    
    print(f"\nğŸ” DOSYA KONTROLLERI:")
    print(f"  {'âœ…' if os.path.exists(json_path) else 'âŒ'} JSON dosyasÄ± mevcut: {json_path}")
    print(f"  {'âœ…' if os.path.exists(images_dir) else 'âŒ'} GÃ¶rsel klasÃ¶rÃ¼ mevcut: {images_dir}")
    print(f"  {'âœ…' if os.path.exists(model_path) else 'âŒ'} Model dosyasÄ± mevcut: {model_path}")
    
    if os.path.exists(images_dir):
        image_count = len([f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
        print(f"  ğŸ“Š GÃ¶rsel sayÄ±sÄ±: {image_count}")
    
    print(f"\nâš™ï¸ EÄÄ°TÄ°M PARAMETRELERÄ°:")
    print(f"  ğŸ“¦ Batch Size: {config['batch_size']}")
    print(f"  ğŸ“ˆ Learning Rate: {config['learning_rate']}")
    print(f"  ğŸ”„ Epochs: {config['epochs']}")
    print(f"  ğŸ’¾ Output KlasÃ¶rÃ¼: {config['output_dir']}")
    print("=" * 60)
    
    # Output dizini oluÅŸtur
    os.makedirs(config['output_dir'], exist_ok=True)
    
    # Trainer oluÅŸtur ve training baÅŸlat
    trainer = ArtCaptionTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main() 